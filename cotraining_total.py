# -*- coding: utf-8 -*-
"""Copy of cotraining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wa22OGT6GpWrLd5Nq9QMjEbaeZ8MybL4
"""

import torch
import torch.nn as nn
import os
import matplotlib.image as mtimage
import matplotlib.pyplot as plt
import numpy as np
import random
import json
import re, os, sys, json, random
from tqdm import tqdm
from PIL import Image
import pandas as pd
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset,DataLoader
import torchvision
from torchvision import transforms
import pandas as pd
import shutil
import torchvision.datasets as dset
import torch.optim as optim
import csv
import torchvision.models as models
import warnings
warnings.filterwarnings("ignore")

torch.cuda.empty_cache()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def find_classes(dir):

    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]

    classes.sort()

    class_to_idx = {classes[i]: i for i in range(len(classes))}
    return classes, class_to_idx

def make_dataset(dir, class_to_idx):
    images = []
    dir = os.path.expanduser(dir)
    for target in sorted(os.listdir(dir)):
        d = os.path.join(dir, target)
        if not os.path.isdir(d):
            continue

        for root, _, fnames in sorted(os.walk(d)):
            for fname in sorted(fnames):
                path = os.path.join(root, fname)
                item = (path, class_to_idx[target])
                images.append(item)
    return images

def pil_loader(path):
    with open(path, 'rb') as f:
        with Image.open(f) as img:
            return img.convert('RGB')


def accimage_loader(path):
    import accimage
    try:
        return accimage.Image(path)
    except IOError:
        # Potentially a decoding problem, fall back to PIL.Image
        return pil_loader(path)


def default_loader(path):
    from torchvision import get_image_backend
    if get_image_backend() == 'accimage':
        return accimage_loader(path)
    else:
        return pil_loader(path)

class DataSet(Dataset):

    def __init__(self, imgs, transform=None, target_transform=None,
                 loader=default_loader):
        self.imgs = imgs
        self.transform = transform
        self.target_transform = target_transform
        self.loader = loader

    def __getitem__(self, index):

        path, target = list(self.imgs.items())[index]
        img = self.loader(path)
        if self.transform is not None:
            img = self.transform(img)
        if self.target_transform is not None:
            target = self.target_transform(target)

        return img, target

    def __len__(self):
        return len(self.imgs)

preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
])

preprocess_b = transforms.Compose([
    transforms.ColorJitter(brightness=0.3, contrast=0.5, saturation=0.3, hue=0.2),
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
])

def co_training(model1, model2, a_dict, b_dict, u_dict, c_dataloader, u_dataloader, max_iter, threshold, batch_s, save_name, save_name_2, weight_name1, weight_name2, mode):
  model1.to(device)
  model2.to(device)
  record=[]
  record_2=[]
  criterion1 = nn.CrossEntropyLoss()
  criterion2 = nn.CrossEntropyLoss()
  optimizer1 = optim.AdamW(model1.parameters(),lr=0.0001,betas=(0.9, 0.999),eps=1e-08,weight_decay=0.001)
  optimizer2 = optim.AdamW(model2.parameters(),lr=0.0001,betas=(0.9, 0.999),eps=1e-08,weight_decay=0.001)

  iteration = 0
  while iteration < max_iter:
    iteration += 1
    best_acc_C1=0
    best_acc_C2=0
    if iteration == 1 and mode=="199":
      epoch=50
    else:
      epoch=30
    print('Iteration:{}'.format(iteration))
    for e in range(epoch):
      a_dataset = DataSet(a_dict, transform=preprocess)
      a_dataloader = DataLoader(a_dataset, batch_size = batch_s, shuffle=True, num_workers=8)

      b_dataset = DataSet(b_dict, transform=preprocess_b)
      b_dataloader = DataLoader(b_dataset, batch_size = batch_s, shuffle=True, num_workers=8)
      #train C1
      model1.train()
      for img_a ,label_a in a_dataloader:
        img_a = img_a.to(torch.float).to(device)
        label_a = label_a.to(device)
        output_a = model1(img_a)
        optimizer1.zero_grad()
        loss_a = criterion1(output_a, label_a)
        loss_a.backward()
        optimizer1.step()

      # #train C2
      model2.train()
      for img_b ,label_b in b_dataloader:
        img_b = img_b.to(torch.float).to(device)
        label_b= label_b.to(device)
        output_b = model2(img_b)
        optimizer2.zero_grad()
        loss_b = criterion2(output_b, label_b)
        loss_b.backward()
        optimizer2.step()

      #valid and update subset datasets
      with torch.no_grad():
        model1.eval()
        model2.eval()
        idx_1 =0
        idx_2 =0
        c = 0
        t = 0
        c1 = 0
        t1 = 0
        c2 = 0
        t2 = 0
        true_t1=0
        true_c1=0
        true_t2=0
        true_c2=0
        predict_ls_np = []
        label_ls_np = []

        for img_c, label_c in c_dataloader:
          img_c = img_c.to(torch.float).to(device)
          label_c=label_c.to(device)

          # true_c1 predict u
          true_predict_c1 = model1(img_c)
          true_prob_c1 = torch.softmax(true_predict_c1, dim=1)
          true_predict1 = torch.argmax(true_prob_c1, dim=1)

          for true_index1 in range(len(true_predict1)):
            true_t1 +=1
            if true_predict1[true_index1]==label_c[true_index1]:
              true_c1+=1

          # true_c1 predict u
          true_predict_c2 = model2(img_c)
          true_prob_c2 = torch.softmax(true_predict_c2, dim=1)
          true_predict2 = torch.argmax(true_prob_c2, dim=1)

          for true_index2 in range(len(true_predict2)):
            true_t2 +=1
            if true_predict2[true_index2]==label_c[true_index2]:
              true_c2+=1
        true_val_acc_1= true_c1 / true_t1 
        if true_val_acc_1 > best_acc_C1:
            best_acc_C1 = true_val_acc_1
            best_weights_C1 = model1.state_dict()
            torch.save(best_weights_C1, weight_name1)

        true_val_acc_2= true_c2 / true_t2
        if true_val_acc_2 > best_acc_C2:
            best_acc_C2 = true_val_acc_2
            best_weights_C2 = model2.state_dict()
            torch.save(best_weights_C2, weight_name2)

        if e == epoch - 1:
          model1.load_state_dict(torch.load(weight_name1))
          model2.load_state_dict(torch.load(weight_name2))

        for img, label in u_dataloader:
          img = img.to(torch.float).to(device)
          label=label.to(device)

          # c1 predict u
          predict_c1 = model1(img)
          prob_c1 = torch.softmax(predict_c1, dim=1)
          predict1 = torch.argmax(prob_c1, dim=1)

          for index1 in range(len(predict1)):
            t1 +=1
            if predict1[index1]==label[index1]:
              c1+=1
              #print
          # classi_acc1=c1 / t1
          # print("e",e,"classi_acc1",classi_acc1)
          #add pseudo label to subsetb

          if e == epoch - 1:
            label_c1 = torch.argmax(torch.tensor(prob_c1),dim=1).cpu().numpy()
            high_confidence_mask_c1 = prob_c1.max(dim=1).values > threshold
            high_confidence_index_c1 = torch.nonzero(high_confidence_mask_c1).squeeze(1).cpu().numpy()
            for index_c1 in high_confidence_index_c1:
              path_c1 = list(u_dict.keys())[index_c1 + idx_1]
              b_dict[path_c1] = label_c1[index_c1]
            idx_1 += batch_s

          # c2 predict u
          predict_c2 = model2(img)
          prob_c2 = torch.softmax(predict_c2, dim=1)
          predict2 = torch.argmax(prob_c2, dim=1)

          for index2 in range(len(predict2)):
            t2 +=1
            if predict2[index2]==label[index2]:
              c2+=1
              #print
          # classi_acc2=c2 / t2
          # print("e",e,"classi_acc2",classi_acc2)

          #add pseudo label to subseta
          if e == epoch - 1:
            label_c2 = torch.argmax(torch.tensor(prob_c2),dim=1).cpu().numpy()
            high_confidence_mask_c2 = prob_c2.max(dim=1).values > threshold
            high_confidence_index_c2 = torch.nonzero(high_confidence_mask_c2).squeeze(1).cpu().numpy()
            for index_c2 in high_confidence_index_c2:
              path_c2 = list(u_dict.keys())[index_c2 + idx_2]
              a_dict[path_c2] = label_c2[index_c2]
            idx_2 += batch_s

          #final result
          prob = (prob_c1+prob_c2)/2
          predict = torch.argmax(prob, dim=1)

          for index in range(len(predict)):
            t+=1
            if predict[index]==label[index]:
              c+=1
        #path changed for test:


        val_acc_1=c1 / t1
        # if val_acc_1 > best_acc_C1:
        #     best_acc_C1 = val_acc_1
        #     best_weights_C1 = model1.state_dict()
        #     torch.save(best_weights_C1, "/content/drive/MyDrive/CS15-2 capstone/cotraining/weight/best_weights_C1_{}.pt".format(select_data))
        # if val_acc_1 < best_acc_C1:
        #     model1.load_state_dict(torch.load("/content/drive/MyDrive/CS15-2 capstone/cotraining/weight/best_weights_C1_{}.pt".format(select_data)))

        val_acc_2=c2 / t2
        # if val_acc_2 > best_acc_C2:
        #     best_acc_C2 = val_acc_2
        #     best_weights_C2 = model2.state_dict()
        #     torch.save(best_weights_C2, "/content/drive/MyDrive/CS15-2 capstone/cotraining/weight/best_weights_C2_{}.pt".format(select_data))
        # if val_acc_2 < best_acc_C2:
        #     model2.load_state_dict(torch.load("/content/drive/MyDrive/CS15-2 capstone/cotraining/weight/best_weights_C2_{}.pt".format(select_data)))
        val_acc_total=c/t

        print("iteration",iteration,"epoch",e,"true_model1 {:.2%}".format(true_val_acc_1),"model1 {:.2%}".format(val_acc_1),"true_model2 {:.2%}".format(true_val_acc_2),"model2 {:.2%}".format(val_acc_2),"model_total {:.2%}".format(val_acc_total))
        record.append({
                    "iteration": iteration,
                    "epoch": e,
                    "true_model1": "{:.2%}".format(true_val_acc_1),
                    "model1": "{:.2%}".format(val_acc_1),
                    "subset_b_size": len(b_dict),
                    "true_model2_accuracy": "{:.2%}".format(true_val_acc_2),
                    "model2_accuracy": "{:.2%}".format(val_acc_2),
                    "subset_a_size": len(a_dict),
                    "total_accuracy": "{:.2%}".format(val_acc_total)
                })
        # 将数据写入CSV文件
        csv_file_path = save_name  # 请替换为实际的文件路径
        csv_columns = ["iteration", "epoch", "true_model1", "model1", "subset_b_size", "true_model2_accuracy", "model2_accuracy", "subset_a_size", "total_accuracy"]

        with open(csv_file_path, 'w', newline='') as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=csv_columns)
            writer.writeheader()
            for row in record:
                writer.writerow(row)

        if e == epoch - 1:
            print('The length of the dictionary for subset b:{}'.format(len(b_dict)))
            print("Accuracy of model1 {:.2%}".format(val_acc_1))
            print('The length of the dictionary for subset a:{}'.format(len(a_dict)))
            print("Accuracy of model2 {:.2%}".format(val_acc_2))
            print("Accuracy of final result {:.2%}".format(val_acc_total))
            record_2.append({
                    "The length of the dictionary for subset b":"{}".format(len(b_dict)),
                    "Accuracy of model1": "{:.2%}".format(val_acc_1),
                    "The length of the dictionary for subset a": "{}".format(len(a_dict)),
                    "Accuracy of model2": "{:.2%}".format(val_acc_2),
                    "Accuracy of final result": "{:.2%}".format(val_acc_total),
                })
            csv_file_path_2 = save_name_2  # 请替换为实际的文件路径
            csv_columns_2 = ["The length of the dictionary for subset b", 
                           "Accuracy of model1", 
                           "The length of the dictionary for subset a", 
                           "Accuracy of model2", 
                           "Accuracy of final result", 
                           ]

            with open(csv_file_path_2, 'w', newline='') as csv_file_2:
                writer_2 = csv.DictWriter(csv_file_2, fieldnames=csv_columns_2)
                writer_2.writeheader()
                for row_2 in record_2:
                    writer_2.writerow(row_2)

def load_parameter(pretrained_model_path, model):
    # 加载预训练权重
    pretrained_dict = torch.load(pretrained_model_path)

    # 获取当前模型的字典
    model_dict = model.state_dict()

    # 从预训练字典中筛选出当前模型参数需要的部分
    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}

    # 更新当前模型的参数
    model_dict.update(pretrained_dict)

    # 加载到模型中
    model.load_state_dict(model_dict)

    return model


def run_code(root_label, root_unlabel, root_model, out_features, co_training_iteration, threshold, batchsize, save_name, save_name_2, weight_name1, weight_name2, mode):
  root_label = root_label
  root_unlabel = root_unlabel

  classes, class_to_idx = find_classes(root_label)

  a_dict = dict(make_dataset(root_label, class_to_idx))
  b_dict = dict(make_dataset(root_label, class_to_idx))
  c_dict = dict(make_dataset(root_label, class_to_idx))
  u_dict = dict(make_dataset(root_unlabel, class_to_idx))

  c_dataset = DataSet(c_dict, transform=preprocess)
  c_dataloader = DataLoader(c_dataset, batch_size=batchsize, shuffle=False, num_workers=8)

  u_dataset = DataSet(u_dict, transform=preprocess)
  u_dataloader = DataLoader(u_dataset, batch_size=batchsize, shuffle=False, num_workers=8)


  C1 = models.resnet50(pretrained=False)
  C1 = load_parameter(root_model, C1)
  C1.fc = nn.Linear(in_features=2048, out_features=out_features, bias=True)

  C2 = models.resnet50(pretrained=False)
  C2 = load_parameter(root_model, C2)
  C2.fc = nn.Linear(in_features=2048, out_features=out_features, bias=True)

  co_training(C1, 
              C2, 
              a_dict, 
              b_dict, 
              u_dict, 
              c_dataloader, 
              u_dataloader, 
              max_iter = co_training_iteration, 
              threshold = threshold, 
              batch_s = batchsize, 
              save_name = save_name,
              save_name_2 = save_name_2,
              weight_name1 = weight_name1,
              weight_name2 = weight_name2,   
              mode=mode
              )



# Change it depending on your equipment
batchsize = 128 #256

co_training_iteration = 5  
threshold=0.98
weight_name1 = "/root/autodl-tmp/weight/best_weights_C1.pth"
weight_name2 = "/root/autodl-tmp/weight/best_weights_C2.pth"








# Need to change the path

# # brain 1:99
# brain_root_label_199 = r"/root/brain_tumor_199_1090/99-1/train"
# brain_root_unlabel_199 = r"/root/brain_tumor_199_1090/99-1/test"
# brain_root_model_199 = '/root/autodl-tmp/MOCO_pretrained_model/brain_tumor_resnet.pth'
# brain_out_features_199=2
# save_name = "/root/autodl-tmp/self_cotraining_result/moco/moco_brain_199_0.98.csv"
# save_name_2 = "/root/autodl-tmp/self_cotraining_result/moco/moco_brain_199_0.98_final_step_of_iteration.csv"
# mode="199"

# run_code(brain_root_label_199, 
#          brain_root_unlabel_199,
#          brain_root_model_199,
#          brain_out_features_199,
#          co_training_iteration,
#          threshold,
#          batchsize,
#          save_name,
#          save_name_2,
#          weight_name1,
#          weight_name2,
#          mode
#          )

#brain 10:90
brain_root_label_1090 = r"/root/brain_tumor_199_1090/90-10/train"
brain_root_unlabel_1090 = r"/root/brain_tumor_199_1090/90-10/test"
brain_root_model_1090 = '/root/autodl-tmp/MOCO_pretrained_model/brain_tumor_resnet.pth'
brain_out_features_1090=2
save_name = "/root/autodl-tmp/self_cotraining_result/moco/moco_brain_1090_0.98.csv"
save_name_2 = "/root/autodl-tmp/self_cotraining_result/moco/moco_brain_1090_0.98_final_step_of_iteration.csv"
mode="1090"

run_code(brain_root_label_1090, 
         brain_root_unlabel_1090,
         brain_root_model_1090,
         brain_out_features_1090,
         co_training_iteration,
         threshold,
         batchsize,
         save_name,
         save_name_2,
         weight_name1,
         weight_name2,
         mode
         )



#covid 1:99
covid_root_label_199 = r"/root/covid/99_1/train"
covid_root_unlabel_199 = r"/root/covid/99_1/test"
covid_root_model_199 = '/root/autodl-tmp/MOCO_pretrained_model/covid_resnet.pth'
covid_out_features_199=3
save_name = "/root/autodl-tmp/self_cotraining_result/moco/moco_covid_199_0.98.csv"
save_name_2 = "/root/autodl-tmp/self_cotraining_result/moco/moco_covid_199_0.98_final_step_of_iteration.csv"
mode="199"

run_code(covid_root_label_199, 
         covid_root_unlabel_199,
         covid_root_model_199,
         covid_out_features_199,
         co_training_iteration,
         threshold,
         batchsize,
         save_name,
         save_name_2,
         weight_name1,
         weight_name2,
         mode
         )


#covid 10:90
covid_root_label_1090 = r"/root/covid/90_10/train"
covid_root_unlabel_1090 = r"/root/covid/90_10/test"
covid_root_model_1090 = '/root/autodl-tmp/MOCO_pretrained_model/covid_resnet.pth'
covid_out_features_1090=3
save_name = "/root/autodl-tmp/self_cotraining_result/moco/moco_covid_1090_0.98.csv"
save_name_2 = "/root/autodl-tmp/self_cotraining_result/moco/moco_covid_1090_0.98_final_step_of_iteration.csv"
mode="1090"

run_code(covid_root_label_1090, 
         covid_root_unlabel_1090,
         covid_root_model_1090,
         covid_out_features_1090,
         co_training_iteration,
         threshold,
         batchsize,
         save_name,
         save_name_2,
         weight_name1,
         weight_name2,
         mode
         )


#cancer 1:99
cancer_root_label_199 = r"/root/cancer_199_1090/99_1/train"
cancer_root_unlabel_199 = r"/root/cancer_199_1090/99_1/test"
cancer_root_model_199 = '/root/autodl-tmp/MOCO_pretrained_model/cancer_resnet.pth'
cancer_out_features_199=2
save_name = "/root/autodl-tmp/self_cotraining_result/moco/moco_cancer_199_0.98.csv"
save_name_2 = "/root/autodl-tmp/self_cotraining_result/moco/moco_cancer_199_0.98_final_step_of_iteration.csv"
mode="199"

run_code(cancer_root_label_199, 
         cancer_root_unlabel_199,
         cancer_root_model_199,
         cancer_out_features_199,
         co_training_iteration,
         threshold,
         batchsize,
         save_name,
         save_name_2,
         weight_name1,
         weight_name2,
         mode
         )


#cancer 10:90
cancer_root_label_1090 = r"/root/cancer_199_1090/90_10/train"
cancer_root_unlabel_1090 = r"/root/cancer_199_1090/90_10/test"
cancer_root_model_1090 = '/root/autodl-tmp/MOCO_pretrained_model/cancer_resnet.pth'
cancer_out_features_1090=2
save_name = "/root/autodl-tmp/self_cotraining_result/moco/moco_cancer_1090_0.98.csv"
save_name_2 = "/root/autodl-tmp/self_cotraining_result/moco/moco_cancer_1090_0.98_final_step_of_iteration.csv"
mode="1090"

run_code(cancer_root_label_1090, 
         cancer_root_unlabel_1090,
         cancer_root_model_1090,
         cancer_out_features_1090,
         co_training_iteration,
         threshold,
         batchsize,
         save_name,
         save_name_2,
         weight_name1,
         weight_name2,
         mode
         )







# # Need to change the path

# # brain 1:99
# brain_root_label_199 = r"/root/brain_tumor_199_1090/99-1/train"
# brain_root_unlabel_199 = r"/root/brain_tumor_199_1090/99-1/test"

# brain_classes_199, brain_class_to_idx_199 = find_classes(brain_root_label_199)

# brain_a_dict_199 = dict(make_dataset(brain_root_label_199, brain_class_to_idx_199))
# brain_b_dict_199 = dict(make_dataset(brain_root_label_199, brain_class_to_idx_199))
# brain_c_dict_199 = dict(make_dataset(brain_root_label_199, brain_class_to_idx_199))
# brain_u_dict_199 = dict(make_dataset(brain_root_unlabel_199, brain_class_to_idx_199))

# brain_c_dataset_199 = DataSet(brain_c_dict_199, transform=preprocess)
# brain_c_dataloader_199 = DataLoader(brain_c_dataset_199, batch_size=batchsize, shuffle=False, num_workers=8)

# brain_u_dataset_199 = DataSet(brain_u_dict_199, transform=preprocess)
# brain_u_dataloader_199 = DataLoader(brain_u_dataset_199, batch_size=batchsize, shuffle=False, num_workers=8)


# C1_brain_199 = models.resnet50(pretrained=False)
# C1_brain_199 = load_parameter('/root/autodl-tmp/MOCO_pretrained_model/brain_tumor_resnet.pth', C1_brain_199)
# C1_brain_199.fc = nn.Linear(in_features=2048, out_features=2, bias=True)

# C2_brain_199 = models.resnet50(pretrained=False)
# C2_brain_199 = load_parameter('/root/autodl-tmp/MOCO_pretrained_model/brain_tumor_resnet.pth', C2_brain_199)
# C2_brain_199.fc = nn.Linear(in_features=2048, out_features=2, bias=True)

# co_training(C1_brain_199, 
#             C2_brain_199, 
#             brain_a_dict_199, 
#             brain_b_dict_199, 
#             brain_u_dict_199, 
#             brain_c_dataloader_199, 
#             brain_u_dataloader_199, 
#             max_iter = co_training_iteration, 
#             threshold = 0.98, 
#             batch_s = batchsize, 
#             save_name = "/root/autodl-tmp/self_cotraining_result/byol/byol_brain_199_0.98.csv",
#             save_name_2 = "/root/autodl-tmp/self_cotraining_result/byol/byol_brain_199_0.98_final_step_of_iteration.csv",
#             weight_name1 = "/root/autodl-tmp/weight/best_weights_C1.pth",
#             weight_name2 = "/root/autodl-tmp/weight/best_weights_C2.pth",   
#             mode="199"
#             )


# #brain 10:90
# brain_root_label_1090 = r"/root/brain_tumor_199_1090/90-10/train"
# brain_root_unlabel_1090 = r"/root/brain_tumor_199_1090/90-10/test"

# brain_classes_1090, brain_class_to_idx_1090 = find_classes(brain_root_label_1090)

# brain_a_dict_1090 = dict(make_dataset(brain_root_label_1090, brain_class_to_idx_1090))
# brain_b_dict_1090 = dict(make_dataset(brain_root_label_1090, brain_class_to_idx_1090))
# brain_c_dict_1090 = dict(make_dataset(brain_root_label_1090, brain_class_to_idx_1090))
# brain_u_dict_1090 = dict(make_dataset(brain_root_unlabel_1090, brain_class_to_idx_1090))

# brain_c_dataset_1090 = DataSet(brain_c_dict_1090, transform=preprocess)
# brain_c_dataloader_1090 = DataLoader(brain_c_dataset_1090, batch_size=batchsize, shuffle=False, num_workers=8)

# brain_u_dataset_1090= DataSet(brain_u_dict_1090, transform=preprocess)
# brain_u_dataloader_1090 = DataLoader(brain_u_dataset_1090, batch_size=batchsize, shuffle=False, num_workers=8)

# C1_brain_1090 = models.resnet50(pretrained=False)
# C1_brain_1090 = load_parameter('/root/autodl-tmp/MOCO_pretrained_model/brain_tumor_resnet.pth', C1_brain_1090)
# C1_brain_1090.fc = nn.Linear(in_features=2048, out_features=2, bias=True)

# C2_brain_1090 = models.resnet50(pretrained=False)
# C2_brain_1090 = load_parameter('/root/autodl-tmp/MOCO_pretrained_model/brain_tumor_resnet.pth', C2_brain_1090)
# C2_brain_1090.fc = nn.Linear(in_features=2048, out_features=2, bias=True)

# co_training(C1_brain_1090, 
#             C2_brain_1090, 
#             brain_a_dict_1090, 
#             brain_b_dict_1090, 
#             brain_u_dict_1090, 
#             brain_c_dataloader_1090, 
#             brain_u_dataloader_1090, 
#             max_iter = co_training_iteration, 
#             threshold = 0.98, 
#             batch_s = batchsize, 
#             save_name = "/root/autodl-tmp/self_cotraining_result/byol/byol_brain_1090_0.98.csv",
#             save_name_2 = "/root/autodl-tmp/self_cotraining_result/byol/byol_brain_1090_0.98_final_step_of_iteration.csv",
#             weight_name1 = "/root/autodl-tmp/weight/best_weights_C1.pth",
#             weight_name2 = "/root/autodl-tmp/weight/best_weights_C2.pth",   
#             mode="1090"
#             )






# #covid 1:99
# covid_root_label_199 = r"/root/covid/99_1/train"
# covid_root_unlabel_199 = r"/root/covid/99_1/test"

# covid_classes_199, covid_class_to_idx_199 = find_classes(covid_root_label_199)

# covid_a_dict_199 = dict(make_dataset(covid_root_label_199, covid_class_to_idx_199))
# covid_b_dict_199 = dict(make_dataset(covid_root_label_199, covid_class_to_idx_199))
# covid_c_dict_199 = dict(make_dataset(covid_root_label_199, covid_class_to_idx_199))
# covid_u_dict_199 = dict(make_dataset(covid_root_unlabel_199, covid_class_to_idx_199))

# covid_c_dataset_199 = DataSet(covid_c_dict_199, transform=preprocess)
# covid_c_dataloader_199 = DataLoader(covid_c_dataset_199, batch_size=batchsize, shuffle=False, num_workers=8)

# covid_u_dataset_199 = DataSet(covid_u_dict_199, transform=preprocess)
# covid_u_dataloader_199 = DataLoader(covid_u_dataset_199, batch_size=batchsize, shuffle=False, num_workers=8)


# C1_covid_199 = models.resnet50(pretrained=False)
# C1_covid_199 = load_parameter('/root/autodl-tmp/MOCO_pretrained_model/covid_resnet.pth', C1_covid_199)
# C1_covid_199.fc = nn.Linear(in_features=2048, out_features=3, bias=True)

# C2_covid_199 = models.resnet50(pretrained=False)
# C2_covid_199 = load_parameter('/root/autodl-tmp/MOCO_pretrained_model/covid_resnet.pth', C2_covid_199)
# C2_covid_199.fc = nn.Linear(in_features=2048, out_features=3, bias=True)

# co_training(C1_covid_199, 
#             C2_covid_199, 
#             covid_a_dict_199, 
#             covid_b_dict_199, 
#             covid_u_dict_199, 
#             covid_c_dataloader_199, 
#             covid_u_dataloader_199, 
#             max_iter = co_training_iteration, 
#             threshold = 0.98, 
#             batch_s = batchsize, 
#             save_name = "/root/autodl-tmp/self_cotraining_result/byol/byol_covid_199_0.98.csv",
#             save_name_2 = "/root/autodl-tmp/self_cotraining_result/byol/byol_covid_199_0.98_final_step_of_iteration.csv",
#             weight_name1 = "/root/autodl-tmp/weight/best_weights_C1.pth",
#             weight_name2 = "/root/autodl-tmp/weight/best_weights_C2.pth",   
#             mode="199"
#             )

# #covid 10:90
# covid_root_label_1090 = r"/root/covid/90_10/train"
# covid_root_unlabel_1090 = r"/root/covid/90_10/test"

# covid_classes_1090, covid_class_to_idx_1090 = find_classes(covid_root_label_1090)

# covid_a_dict_1090 = dict(make_dataset(covid_root_label_1090, covid_class_to_idx_1090))
# covid_b_dict_1090 = dict(make_dataset(covid_root_label_1090, covid_class_to_idx_1090))
# covid_c_dict_1090 = dict(make_dataset(covid_root_label_1090, covid_class_to_idx_1090))
# covid_u_dict_1090 = dict(make_dataset(covid_root_unlabel_1090, covid_class_to_idx_1090))

# covid_c_dataset_1090 = DataSet(covid_c_dict_1090, transform=preprocess)
# covid_c_dataloader_1090 = DataLoader(covid_c_dataset_1090, batch_size=batchsize, shuffle=False, num_workers=8)

# covid_u_dataset_1090= DataSet(covid_u_dict_1090, transform=preprocess)
# covid_u_dataloader_1090 = DataLoader(covid_u_dataset_1090, batch_size=batchsize, shuffle=False, num_workers=8)

# C1_covid_1090 = models.resnet50(pretrained=False)
# C1_covid_1090 = load_parameter('/root/autodl-tmp/MOCO_pretrained_model/covid_resnet.pth', C1_covid_1090)
# C1_covid_1090.fc = nn.Linear(in_features=2048, out_features=3, bias=True)

# C2_covid_1090 = models.resnet50(pretrained=False)
# C2_covid_1090 = load_parameter('/root/autodl-tmp/MOCO_pretrained_model/covid_resnet.pth', C2_covid_1090)
# C2_covid_1090.fc = nn.Linear(in_features=2048, out_features=3, bias=True)

# co_training(C1_covid_1090, 
#             C2_covid_1090, 
#             covid_a_dict_1090, 
#             covid_b_dict_1090, 
#             covid_u_dict_1090, 
#             covid_c_dataloader_1090, 
#             covid_u_dataloader_1090, 
#             max_iter = co_training_iteration, 
#             threshold = 0.98, 
#             batch_s = batchsize, 
#             save_name = "/root/autodl-tmp/self_cotraining_result/byol/byol_covid_1090_0.98.csv",
#             save_name_2 = "/root/autodl-tmp/self_cotraining_result/byol/byol_covid_1090_0.98_final_step_of_iteration.csv",
#             weight_name1 = "/root/autodl-tmp/weight/best_weights_C1.pth",
#             weight_name2 = "/root/autodl-tmp/weight/best_weights_C2.pth",   
#             mode="1090"
#             )


# #cancer 1:99
# cancer_root_label_199 = r"/root/cancer_199_1090/99_1/train"
# cancer_root_unlabel_199 = r"/root/cancer_199_1090/99_1/test"

# cancer_classes_199, cancer_class_to_idx_199 = find_classes(cancer_root_label_199)

# cancer_a_dict_199 = dict(make_dataset(cancer_root_label_199, cancer_class_to_idx_199))
# cancer_b_dict_199 = dict(make_dataset(cancer_root_label_199, cancer_class_to_idx_199))
# cancer_c_dict_199 = dict(make_dataset(cancer_root_label_199, cancer_class_to_idx_199))
# cancer_u_dict_199 = dict(make_dataset(cancer_root_unlabel_199, cancer_class_to_idx_199))

# cancer_c_dataset_199 = DataSet(cancer_c_dict_199, transform=preprocess)
# cancer_c_dataloader_199 = DataLoader(cancer_c_dataset_199, batch_size=batchsize, shuffle=False, num_workers=8)

# cancer_u_dataset_199 = DataSet(cancer_u_dict_199, transform=preprocess)
# cancer_u_dataloader_199 = DataLoader(cancer_u_dataset_199, batch_size=batchsize, shuffle=False, num_workers=8)


# C1_cancer_199 = models.resnet50(pretrained=False)
# C1_cancer_199 = load_parameter('/root/autodl-tmp/MOCO_pretrained_model/cancer_resnet.pth', C1_cancer_199)
# C1_cancer_199.fc = nn.Linear(in_features=2048, out_features=2, bias=True)

# C2_cancer_199 = models.resnet50(pretrained=False)
# C2_cancer_199 = load_parameter('/root/autodl-tmp/MOCO_pretrained_model/cancer_resnet.pth', C2_cancer_199)
# C2_cancer_199.fc = nn.Linear(in_features=2048, out_features=2, bias=True)

# co_training(C1_cancer_199, 
#             C2_cancer_199, 
#             cancer_a_dict_199, 
#             cancer_b_dict_199, 
#             cancer_u_dict_199, 
#             cancer_c_dataloader_199, 
#             cancer_u_dataloader_199, 
#             max_iter = co_training_iteration, 
#             threshold = 0.98, 
#             batch_s = batchsize, 
#             save_name = "/root/autodl-tmp/self_cotraining_result/byol/byol_cancer_199_0.98.csv",
#             save_name_2 = "/root/autodl-tmp/self_cotraining_result/byol/byol_cancer_199_0.98_final_step_of_iteration.csv",
#             weight_name1 = "/root/autodl-tmp/weight/best_weights_C1.pth",
#             weight_name2 = "/root/autodl-tmp/weight/best_weights_C2.pth",   
#             mode="199"
#             )


# #cancer 10:90
# cancer_root_label_1090 = r"/root/cancer_199_1090/90_10/train"
# cancer_root_unlabel_1090 = r"/root/cancer_199_1090/90_10/test"

# cancer_classes_1090, cancer_class_to_idx_1090 = find_classes(cancer_root_label_1090)

# cancer_a_dict_1090 = dict(make_dataset(cancer_root_label_1090, cancer_class_to_idx_1090))
# cancer_b_dict_1090 = dict(make_dataset(cancer_root_label_1090, cancer_class_to_idx_1090))
# cancer_c_dict_1090 = dict(make_dataset(cancer_root_label_1090, cancer_class_to_idx_1090))
# cancer_u_dict_1090 = dict(make_dataset(cancer_root_unlabel_1090, cancer_class_to_idx_1090))

# cancer_c_dataset_1090 = DataSet(cancer_c_dict_1090, transform=preprocess)
# cancer_c_dataloader_1090 = DataLoader(cancer_c_dataset_1090, batch_size=batchsize, shuffle=False, num_workers=8)

# cancer_u_dataset_1090= DataSet(cancer_u_dict_1090, transform=preprocess)
# cancer_u_dataloader_1090 = DataLoader(cancer_u_dataset_1090, batch_size=batchsize, shuffle=False, num_workers=8)

# C1_cancer_1090 = models.resnet50(pretrained=False)
# C1_cancer_1090 = load_parameter('/root/autodl-tmp/MOCO_pretrained_model/cancer_resnet.pth', C1_cancer_1090)
# C1_cancer_1090.fc = nn.Linear(in_features=2048, out_features=2, bias=True)

# C2_cancer_1090 = models.resnet50(pretrained=False)
# C2_cancer_1090 = load_parameter('/root/autodl-tmp/MOCO_pretrained_model/cancer_resnet.pth', C2_cancer_1090)
# # C2_cancer_1090.fc = nn.Linear(in_features=2048, out_features=2, bias=True)

# co_training(C1_cancer_1090, 
#             C2_cancer_1090, 
#             cancer_a_dict_1090, 
#             cancer_b_dict_1090, 
#             cancer_u_dict_1090, 
#             cancer_c_dataloader_1090, 
#             cancer_u_dataloader_1090, 
#             max_iter = co_training_iteration, 
#             threshold = 0.98, 
#             batch_s = batchsize, 
#             save_name = "/root/autodl-tmp/self_cotraining_result/byol/byol_cancer_1090_0.98.csv",
#             save_name_2 = "/root/autodl-tmp/self_cotraining_result/byol/byol_cancer_1090_0.98_final_step_of_iteration.csv",
#             weight_name1 = "/root/autodl-tmp/weight/best_weights_C1.pth",
#             weight_name2 = "/root/autodl-tmp/weight/best_weights_C2.pth",   
#             mode="1090"
#             )